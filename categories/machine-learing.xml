<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>Land Of Plenty (Posts about Machine Learing)</title><link>https://renewang.github.io/land_of_plenty/</link><description></description><atom:link rel="self" href="https://renewang.github.io/land_of_plenty/categories/machine-learing.xml" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Thu, 30 Nov 2017 13:26:34 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>On The Second Thought ...</title><link>https://renewang.github.io/land_of_plenty/posts/on-the-second-thought/</link><dc:creator>Rene Wang</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;As mentioned in previous post, the commonly used baseline classifier, Multi-nomial Naive Bayes, based on Bag of Words assumption, has a corresponding graphical model interpretation. This graphical model has one root connecting to children nodes. These children nodes, also leaves in the graphical model, representing conditionally independent word features to the root, the sentiment label. (&lt;a href="http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf"&gt;Zhang H, 2004&lt;/a&gt;). The naivity of this assumption fails to fulfill the real world applicaitons. To further step forward from this base model, the connections between different features or edges need to be built on Naive Bayes graph model. In addition to relaxing conditionally independent assumptions, the shift from treating word as discrete count to continuous vector will be another feast to get closer real world model. However, considering the monstrous dimensionality constructed from sparse vocabulary space in natural language, building connections is an intimidating work due to computation overhead. Using binary parsing tree as in Socher et al.'s work is a clever way to limit search space with the syntactic order constrain (&lt;a href="http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf"&gt;Socher et al, 2013&lt;/a&gt;). Using latent semantics analysis such as truncated SVD to remap word count into continuous vector space will be explored in the following posts.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://renewang.github.io/land_of_plenty/posts/on-the-second-thought/"&gt;Read moreâ€¦&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Machine Learing</category><category>NLP</category><guid>https://renewang.github.io/land_of_plenty/posts/on-the-second-thought/</guid><pubDate>Tue, 31 Jan 2017 03:15:32 GMT</pubDate></item></channel></rss>